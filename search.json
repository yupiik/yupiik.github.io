[{"lang":"en","lvl2":"Community","lvl3":"Apache Software Foundation","text":"Part of our team is strongly involved in open source software foundation.\nApache committed contributor and member\nThe ASF develops, shepherds, and incubates hundreds of freely-available, enterprise-grade projects that serve as the backbone for some of the most visible and widely used applications in computing today. Through the ASF's merit-based process known as \"The Apache Way,\" more than 800 individual volunteer Members and 7,800+ code Committers across six continents successfully collaborate on innovations in Artificial Intelligence and Deep Learning, Big Data, Build Management, Cloud Computing, Content Management, DevOps, IoT and Edge Computing, Mobile, Servers, and Web Frameworks, among other categories.\nVisit","title":"Community","url":"/community.html"},{"lang":"en","lvl2":"Configuration SDK, what is it?\nConfiguration SDK: a first (typescript) example\nKubernetes case with CDK8S\nIntegration with a CI/CD pipeline\nConfiguration SDK and migrations\nConclusion","text":", Romain Manni-Bucau , 2021-06-15, 10 min and 59 sec read\nSDK can have various forms and we get the same kind of packaging for configuration oriented SDK:\n- A library you integrate with your \"application\"/deployment code providing Application Programming Interface entry points (API), - A distribution bringing tools (Command Line Interface oriented or UI oriented) and documentation, - A full software suite (i.e. the library but also a testing toolkit), - A docker image prebundling one or multiple of the previous images, - And much more flavor.\nAt the end the goal is to share some existing code to enable consumers (users) to integrate the underlying software faster.\nIn the case of configuration it is exactly the same but the provided API enables to generate configuration files or their content.\nTo illustrate this concept, let's take a simple example:\n- Assume we have a software which is configured thanks to a com.yupiik.demo.json configuration file - The previous JSON generally looks like: +\nQuite quickly, we can envision the a binding for this configuration.\nWe will use typescript to illustrate this example but Python, ruby, PHP, Java, ..., and even JSON with its JSON-Schema would have worked too.\nHere is what a library can provide as configuration binding:\nNothing crazy but it then enables to generate the configuration from this model:\nThis does not look way better than before but it is actually quite better than writing the JSON manually:\nthe required fields will be enforced and compilation will fail if not respected,\nyou get completion on the attributes\nConfiguration can be injected anywhere (console, file, nested in another configuration file, enterprise storage, git, ...),\nWe can generate multiple configurations at onces (for ex: one per environment using the same code).\nThe next immediate benefit from such SDK is to be able to \"code\". Since now we are in code land, we can replace some static parts by code with logic:\nThe goal of this example is to make you feel what a configuration SDK can give you as power. Common next steps are:\nRead the environment configuration from a \"deployment repository\" (it can be a git repository per environment/application with the related permission management or a database-like storage),\nIf the configuration has arrays/lists, you can make it way easier,\nIf the configuration is more complex than the number of inputs (quite common in proxies/gateways cases where input is the target proxy host and rest is quite static), it becomes easy to do a function to hide the complexity and just manipulate the ops data.\nit is important to have an infrastructure storage which enables auditing (who did what).\nSome configuration SDK come with a specific DSL but it is generally worth doing a company/team DSL which encapsulates the software specificities to make it company oriented: you always better know what you do than what others do:\nWith such a DSL - you can publish yourself too as a library on your enterprise NPM registry for example, you increase a lot the sharing between teams/teammates will reduces a lot the entrycost when one of your workers move from one application to another. It also limits a lot the errors or forgotten points (like forgetting to configure the logs in JSON for example).\nKubernetes uses the phylosophy presented in this post with its Cloud Development Kit ). It supports the main ops languages except ruby which tends to be less popular these days: Typescript, JavaScript, Python, and Java.\nA simple example of CDK usage is to create a ConfigMap hosting the generated configuration and injecting it into a deployment.\nThe first step to do it is to import the needed dependencies:\nThen we define our Chart which aggregates the different components of our deployment:\nFrom there, still in the Chart constructor, we can define our components (they are attached thanks the first paramter which is the chart itself).\nThe first one is a ConfigMap:\nThen we create a deployment which is, in this case, nothing more than the aggregation of a Volume - with our ConfigMap mounted inside - and a Container:\nFinally, when we fully defined our model we can create an application - App - containing our specifications and dump it on the disk as YAML a file:\nNow our YAML generator is fully coded and integrated with our configuration generator, we can run the program and we will get a dist/demo.k8s.yaml file with this content:\nAt that stage the last remaning task is to run kubectl or bundlebee on the generated YAML: kubectl apply -f dist/demo.k8s.yaml.\nThere are a lot of strategies to automate previous process execution and it would make this post way too long to detail it all here but note that once previous project is coded, it is quite trivial to integrate it with any CI.\nThe rules are generally something along this rule: when a push/merge is done on branch X (branch name can be environment name or a single branch name like master or main depending how you structure your source repository) execute the deployment.\nThe build steps are generally:\nClone the project\nBuild the project\nRun the generation\n(optional) Test the generated files or code\nExecute the deployment\nHere is a skeleton of Github Actions workflow file using CDK8s:\nin practise, the last step is a bit more complicated and can even be generated from second step in case you want to uninstall some Kubernetes components.\nLast important point is that this workflow is to setup on the configuration repository in general since it is the one with changes which are impacting the production. The generation code can be hosted in the same repository or not - it is really up to you - but it is recommended to either use a library for the generation - limiting a lot the hosted code in the configuration repository - or custom github action which will run the generation properly. If you don't do it on the configuration repository, you will deploy each time you modify your generation code. It will work in some cases but as soon as your deployment code will be stable it will not be what you want since deployment will never be triggered on configuration changes.\nWhen migrating from one version to another one, in particular when the new version is a new major, it can be hard to not loose configuration or be perfectly aware of the changes.\nWith a configuration SDK and coded configuration as we saw previously, this task becomes a standard coding task which will be able to leverage all the well known related tools:\nSCM to identify the differences (git for example),\nThe SDK will enable to validate the new configuration automatically,\nThanks to the \"function\" you can create to share code for parts of the configuration, you can migrate faster (no need to do it per environment if you already managed it with functions for example),\nIt is less error prone if you code your configuration value lookups from a data repository (values not being hardcoded, no risk to wrongly copy/paste them for example).\nIn this post we saw that being able to \"code\" its configuration is a key feature to integrate with CI/CD. It enables to reduce the errors, validate quickly its configuration and do migrations was easier since the \"new\" configuration will be revalidated once the SDK upgraded.\nFrom the same author:\n\nRomain Manni-Bucau\nIn the same category:\nInfrastructure\nPrevious\nAll posts\nNext","title":"Configuration SDK to automate Kubernetes deployments","url":"/blog/configuration-sdk-to-automate-kubernetes-deployments.html"},{"lang":"en","lvl2":"Debug a container in IDE\nBasic Example","text":"Gorhan Hudebine , 2022-04-29, 1 min and 45 sec read\nDocker let you run applications or webapps under containers.\nWhen working with automating deployment like Kubernetes , some bug or undesired behaviour can happen on a java application inside a container, but not reproducible outside the container.\nIt is quite easy to pass on debug mode when launching a container, by adding to the \"docker run\" command :\nAfter executing the command, the container will be running, waiting for a listener on port 5005 to start. Take your favourite IDE and listen or attach process on port 5005, you're on debug inside of your container.\nCreate a TestDebugMain.java in package \"test\", and put a breakpoint on the \"System.out.println()\" line :\nCreate a folder \"debug-test\", put the TestDebugMain.java under \"debug-test/test/\". Create a Dockerfile under \"debug-test\" :\nHere is the final tree :\nWith the terminal, go under \"test-debug\" folder, and build the docker image \"test-debug\", :\nfinally, let's launch a container with that image, and the debug mode, as explained at the begin of this topic (don't forget to add the breakpoint in your IDE) :\nGo to your IDE, and attach to process at port 5005, and that's it !\nFrom the same author:\n\nGorhan Hudebine\nIn the same category:\nDocker\nPrevious\nAll posts\nNext","title":"Docker/Java - How to debug a container using Java application","url":"/blog/docker-debug-container.html"},{"lang":"en","lvl2":"Context\nTypescript Example","lvl3":"login page\nfrom the login page to the returnURL\nJWT token\nenrich header request with the token\nConclusion","text":"the Meecrogate environment is available so we want to authenticate the users of our frontend application through the Oauth2 server. We will be focusing on how to implement the front end application with Angular. The first step would be to register our client on the Oauth2 server and specify the flow we are interested in. The Oauth2 server offers few flow options:\nauthorization code\nPKCE\nclient credentials\npassword\nrefresh token\nYou can find more information about those different flows in here^ .\nFor this example we will register our client with 'Authorization Code with PKCE' type, so from the front end we will have the following steps to manage:\n. go to the login page (customizable) on the oauth2 server with a return URL . catch the code return when the flow come back to the return URL from the login page . ask for a JWT token with the returned code . use the token to enrich your request to the APIs . refresh when/if needed\nThis example has been implemented with Angular so it is in typescript and can be adapted for react or other frameworks.\nAt the begining either we check if the user isn't authenticated then we directly go to the Oauth2 login page or we can also have a non-logged user access with a login button.\nThe method above creates a codeVerifier and stores it at the browser level, it also stores the returnUrl. The user is then sent to the login page with all those information.\nThe design of the login page can easily be customized through the oauth2 admin portal when you create the client.\nOn our angular application we have a method that will extract the code when the user comes back from the login page to the returnUrl page.\nWe catch the code returned and request for a JWT token.\nLet's have a look at the token before managing it. the token can easily be customized and enriched with other data (e.g. LDAP information ...) but in our case we are mainly interested by the token itself so we can consume protected apis.\nWe are going to store the token at the service level as well as the expiry information so we can refresh the token when needed.\nNow we can call the apis with our token so the user can navigate in our application. We only need to enrich the header of the apis calls with the token information\nNow that we have a token we can call the protected apis with it and eventually renew the token when it get expired.\nWe first create an interceptor to catch all the outgoing requests\nthe intercepted request will be enriched with the token information but we also check if the token is still valid otherwise we refresh it and then process the request with the refreshed token.\nWe now have the authentication process working so the user can access protected apis and display those information in our angular application.\nIn few steps we have been able to secure our web application with the Meecrogate Oauth2 Server. In this example we have been focusing on the PKCE flow but it would be quite simple to adapt the code above for other flows.","title":"How to secure your web application with the Meecrogate OAuth2 server","url":"/blog/secure-frontend-application.html"},{"lang":"en","lvl2":"The operator or chicken-egg issue\nInfrastructure as code (IaC)\nBundleBee: the enterprise Kubernetes deployer!\nConclusion","text":", Romain Manni-Bucau , 2024-01-31, 7 min read\nThe first myth to understand and kill is that Kubernetes operators are not intended to help with the deployment. They are great tools to enable a specific DSL and abstract some details in some cases, they are awesome for insanely complicated and dynamic deployments, but they are by design not very welcomed in most of the case because they are required to keep watching or querying Kubernetes API plus being up in the cluster.\nWhat does it mean? Assume you have a cluster of two nodes of 4 CPU and 4Gi of RAM - let's ignore the OS/kubelet daemon resources for the exercise, then your operator will need some resource since it \"runs\". Reviewing some common operators it is often something like [requests=200m;limits=500m] for the CPU and [requests=250Mi;limits=500Mi] for the RAM - when using a single instance. A common example is that ArgoCD \"default\" configuration will consume [requests=1.250m+1.(50m+10m)+1.10m+1.100m+1.100m;limits=1.500m+5.(100m+50m)+5.50m+1.100m+1.100m](controller+server with ui option+repo+applicationSet+notifications) which makes [requests=520m;limits=1700m]. Now keep in mind you need at least the most consuming instance to be able to rollout a pod (if you have 4 CPU and that 3.1 are taken you can't deploy a new instance needing 1 CPU), then it means you need at least 250m of margin to rollout ArgoCD so overall ArgoCD will require 2CPU in your cluster.\nas always setting the requests a limits value for resources is not always good in Kubernetes, in particular, the CPU limits should rarely be set because it preallocate it and prevents it to be used even when idle, this will make your server requiring way more CPU - but it is often the recommended configuration for operators even if default tends to not set resources (which is worse because you can end up with your operator just not running and keep failing to boot).\nyou can do the same kind of reasoning for the memory, but we'll not in this post for brevity.\nTo rephrase it in a trivial way: from a cluster of 8 available CPU (two nodes with 4 CPU) you now just have a cluster of 6 CPU for your applications. What is important to keep in mind is that any resource is paid in a cluster. It is quite obvious in the cloud but on premise, even if you abstract it with a virtual machines solution like VMWare, it is still resources you have to pre-allocate and not allocate to business applications.\nSo yes operators make it looks like it is easy but:\nIt abstracts even more what you do and can make you loose control on what you deploy,\nIt needs to be installed before you can rely on it - only assumption which is fair is \"Kubernetes is running\" - if you use ArgoCD you can't deploy before having deployed it for example,\nIt consumes resources by design you can prefer to reallocate to applications,\nIt is something more to manage in time (security issues, version rollouts, ...) so more work for your ops team in terms of testing and deployment work.\nConsidering all the above mentioned and the effect-consequences, at Yupiik, we tend to prefer client-only deployment solution rather than..... (add a benchmark) The main big advantage is that you don't need to add any resource pressure on the cluster (which is not its primary scope), CPU, memory but also image garbage collection or storage.\nSince day-0 we thought that infrastructure as code is a key part of any project.\nThe statement:\nIs not something acceptable and therefore being able to bundle the application AND its deployment is a key part to ensure the developers and ops can work together toward a better application for end user with smoother deployments for everybody.\nWe ear a lot about GitOps but without entering inside a solution first it is important to understand the key aspects behind it.\nSeeing the infrastructure as code aims at enabling to use code tools which are advanced and automated on the ops side of our work.\nIn other words, IaC means you can:\n(optionally) generate your deployment,\n(optionally) test your deployment,\nput your deployment in a CI/CD pipeline,\nexecute your deployment automatically based on triggers/conditions (from a manual trigger - a human being clicks on a button, to a rule like production branch was updated),\n(optionally) audit your deployment (CVE for example).\nYupiik BundleBee is a light Java package manager for Kubernetes applications and was designed with the principles of Infrastructure as Code in mind:\nLike the same binary is the result of the same source code, the same infrastructure is the result of the same configuration or definition file.\nIt is a way to:\npackage a Kubernetes deployment recipe (called alveolus in BundleBee semantic),\nto make the deployment dynamic using placeholders - and here no need to learn Go language like with Helm charts,\nto validate the deployment with Junit5 - or any other solution,\nto compare the state of your cluster with the recipe - to identify the differences made manually if any (\"quick fixes\") and ensure both converge to the same state.\nRecently we used Bundlebee for a customer in the Bank industry to bootstrap from scratch a devops stack and a full Kubernetes cluster from dev to production:\nEnvironment management with configuration tuning per env\nIntegration in a standard Apache Maven pipeline\nSecrets injections based on crypto\n~100 apps (cronjobs, deployments, jobs - without cron)\nDev factory setup in 1 command (Gitea, Drone, Mattermost, Gitea-pages)\nPlaceholders extraction for an easier interaction with ops\nLiving documentation of the available configuration\nAuto redeployment and easy rollbacks on need\nWe often combine BundleBee with our generic Maven Plugin (a.k.a. yupiik-tools-maven-plugin) which contains two little gems:\na properties (de)ciphering solution we use to store the placeholder values in our sources (often a git repository) in a secured manner,\na static site generator (a.k.a. minisite) we use to integrate the documentation of our alveoli (recipes), ie the available placeholders but also the diff with the cluster per environment.\nthis post is not about how to make it into practise but more the pillar of our deployment solution, another blog post will come soon to enter more into the technical details of such a pipeline.\nHere is a diagram showing this kind of pipeline:\nIn parallel two other pipelines are generally used. The first one is in the build of the alveolus/recipe itself:\nAnd finally another one in the deployment project (we tend to use another project where permissions are reduced for security reasons):\nThere are a lot of trend and tutorials, good will and examples about how to deploy today. However, a lot is either full marketing content or more about promoting a technical aspect. As usual, the best is to step back and see what is really needed for you and pick your own trade-off.\nIn this post, we saw that there is no free lunch and that a well thought CI/CD pipeline can be worth any operator or runtime. As we saw people moving away from WordPress to embrace static website generation 10 years ago, the same will hopefully slow happen on infrastructure as code for the good.\nBundleBee is a really worth it solution on that aspect which can help you to use the same recipe from dev to production with a high quality validation pipeline (linting, testing, reporting).\nStay tuned for more information on how to make it happening in the coming blog posts!\nThis post is not about how to make it into practise but more the pillar of our deployment solution, another blog post will come soon to enter more into the technical details of such a pipeline.\nFrom the same author:\n\nRomain Manni-Bucau\nIn the same category:\nInfrastructure\nAll posts\nNext","title":"IaC Manage your Kubernetes deployments","url":"/blog/iac-manage-your-kubernetes-deployments.html"},{"lang":"en","lvl2":"JSON-RPC scope\nAnatomy of a JSON-RPC exchange\nBulk handling\nGoing further\nConclusion","lvl3":"JSON-RPC Request\nJSON-RPC Response","text":", Romain Manni-Bucau , 2021-06-14, 8 min and 18 sec read\nJSON-RPC defines a protocol. It enables to unify the business logic under a single pattern with a standard stucture accross the whole system.\nJSON-RPC defines a lightweight RPC protocol. In other words, it defines the way to do a request and the way the response will be sent back to the caller.\nAs a high level RPC (Remote Procedure Call) solution, it is transport agnostic which means it can be used over HTTP (the most common), websocket, plain socket or even a messaging system like Kafka or ActiveMQ.\nIts format is, as the name suggests, plain JSON.\nThe request shape is defined as a JSON with the following list of attributes:\njsonrpc: protocol version, as of today it must be 2.0,\nmethod: the \"endpoint\"/operation to call, business names must not start with rpc. (it is reserved for internal RPC methods),\nparams (optional): it is the method parameters, they can be a list, in such a case the params type is an array and parameters are ordered - it is called by-position - or an object which means the parameters will be named (root attributes being the name of the parameters) - it is called by-name,\nid (optional): it can be omitted, null, a number - normally only integers - or a string. It is coupled with the response to associate the response to its request (same value) but over most connected transports it can be omitted even if not recommended. This is more important for bulk calls (we'll see it later).\nHere is an example of simple, no parameter request:\nA parameterized method using ordered parameter(s) can look like this:\nThe same method using named parameter will look like:\ndepending the JSON-RPC framework you use you will be able to use both parameter options or a single one.\nNotifications are plain JSON-RPC requests but they never have an id attribute. It is supposed to notify the client does not care about the response (client push the data but does not expect an answer).\nmost JSON-RPC frameworks are very tolerant over this high level concept, ensure to check what yours does/enables.\nJSON-RPC reponses are very similar to JSON-RPC requests and define the following attributes:\nresult (only on success): when the call suceeds it contains the response to the request,\nerror (only on failures): it is an object (we'll define the structure just after) which is present when the call failed,\nid: same value as the request id to enable to associate the response to the request.\nThe error field is a JSON object which contains the following fields:\ncode,\nmessage: a short description representing the error - a bit like an exception message,\ndata: a free JSON value giving context about the error (it can be a string, number, object, array, ...).\nHere is a sample success response to previous save-user request:\nAnd here is an error response sample:\nTo optimize the network usage, JSON-RPC specification enabled to bulk the requests. This is one of the cases where using id in requests becomes very important because the server can process the requests concurrently in some cases.\nExcept when the request is invalid - and the response will be a standard error, the request and response will be an array of request/responses as seen previously. The only trick to keep in mind is to match the response based on the identifier of the request and not the order in the array which is not guaranteed by the specification.\nHere is an example of request trying to list users and roles through the same request:\nAnd here is a potential response:\nexamples stay simple in the context of this post but in real applications the listing would use as usual a pagination structure ({total,items} for example).\nImplementations generally provide a MethodRegistry or whatever API enabling you to do a call based on a request object.\nCoupled with the fact parsing a JSON is quite easy, it enabled you to add enriched methods enabling to do more.\nA common example is a bulk like endpoint chaining the calls with a preprocessing of the \"next\" call. This case is really common these days and enables to give the caller some orchestration capabilities (à la GraphQL but more powerful and easier in terms of implementation and integration with any framework/stack/language).\nTo illustrate this example, let's assume we will enrich the bulk handling by supporting a /$extension/patch additional entry in the request object. The idea is to iterate over each request of the incoming array, executes the JSON-RPC method and stores the response in an object (we can modelize it a JSON with an attribute /responses which is the list/array of the previous responses). Before executing the JSON-RPC method it will apply the JSON-Patch in /$extension/patch to the request and execute the method with the result JSON instead of the raw incoming one.\nHere is an example of request with such a logic:\nThe response would be exactly the same as in previous example but the big difference and gain of such a technic is that we chained two calls and the second call used the result from the previous call - to filter the roles to list from the list of users.\nA more complex case would use exactly the same technic to:\nPersist some entity,\nPersist some other entity and link it to previous stored entity (by primary key for example),\nTrigger some action on the last persisted entity.\nAll that in a single call and without having to do a specific endpoint, just CRUD for the entities and the action endpoint.\nIt really opens doors to the client/frontend applications without requiring any investment in terms of backend - no customization of the server but no proxy-like server too to add the missing endpoints for the frontend application.\nThis enriched bulk method is really just a small example of what JSON-RPC enables.\nWhat is important to keep in mind is that it is a very simple protocol which, being based on JSON, can be supported by any server and client. It is really one of the most polyglot solution as of today and outperform GraphQL or alternaitve a lot on that aspect.\nThe other very nice thing with JSON-RPC is that since it is JSON and just about a command oriented registry (the method implementations), it is very easy to extend it with more advanced features. We saw how to enrich it in terms of orchestration but you can also add field filtering quite easily (most trivial implementation is about filtering a JSON) or even optimize bulk-ed requests by collapsing them (doing pushdown on the bulk request, for example merging two SQL requests in one).\nThe last point is that it is transport agnostic so you can use it:\nover HTTP (1, 2, 3) indeed,\nover websockets,\nbut also over messaging systems (notifications and id usage makes a lot of sense there) including Apache ActiveMQ or Apache Kafka,\nor even to implement a command line interface (CLI) since the options will be the request attributes but it is a command oriented design - we do it at Yupiik to leverage our existing backend on some products.\nSo last word is that when you want a very flexible protocol you can invest a bit in your company and be sure it will match any transport, performance and feature, JSON-RPC is a very good bet in today's ecosystem.\nFrom the same author:\n\nRomain Manni-Bucau\nIn the same category:\nTechnology\nPrevious\nAll posts","title":"JSON-RPC presentation","url":"/blog/jsonrpc-protocol-presentation.html"},{"lang":"en","text":"","title":"Project starter","url":"/project-starter.html"},{"lang":"en","lvl2":"JWT and cryptography\nMicroservice and JWT common usage\nReduce CPU and maintenance cost for JWT based applications\nMeecrogate example\nImpact of forwarding data into headers on services\nUse a gateway, don't do it in a service\nConclusion","text":"\nAs a quick reminder, a JSON Web Token (JWT) is composed of three segments:\nEach segment is Base64 URL encoded. The segments composing the JWT are:\nThe JSON header, it contains metadata about the payload and signature (exactly as HTTP headers contain metadata about the HTTP payload - content length, content-type etc...). Generally it is what enables to verify the signature (algorithm, key id), what enables to read the payload (is it compressed or not) etc...,\nThe JSON payload which is an almost free JSON where you can put the data you want to share,\nThe signature which guarantees the token was not modified. It is the signature of the first two segments (concatenated with a separating dot).\nAs you can see, the signature computation is what consumes the most of the CPU since other parts (base64 computation and JSON serialization) are really acceptable/simple.\nThe signature can be of multiple types but the most common are:\nRSA (Rivest–Shamir–Adleman from the name of their creators),\nEC (elliptic curve),\nHmac (hash based message authentication code),\nPS (also known as RSASSA-PSS for RSA Probabilistic Signature Scheme).\nIndeed, in the context of this post we only care about the performances but keep in mind you don't always select the algorithm for its performances. A common example is that if you select Hmac, except being exposed to brute force hacking, you can't share the key to verify the JWT since it would enable to create custom JWT which would be valid from a signature point of view. Then in terms of pure performance, depending if you prefer a fast encryption (signing) or decryption (verifying) you will pick RSA or EC depending other security constraints we will not detail in this particular post.\nWhat we often see in microservice architectures is this kind of deployment:\nWhat is interesting to see is that for a quite common application with an API layer, a frontend and a backend service (in real deployments we often have N backend services and not a single one), we propagate the JWT at least 4 times.\nin real applications the JWT is also propagated to topics/Kafka messages to be able to load the security context which makes it even worse.\nPropagating the JWT is not really crazy since it is mainly propagating a header but before using a JWT you must ensure it is valid otherwise you can not trust its content so each service in this chain will have to:\nEnsure there is a JWT (cheap),\nEnsure the JWT is valid (cheap, mainly base64 and JSON validation on the header),\nEnsure the JWT signature is valid (expensive in terms of CPU),\nEnsure the JWT payload/content is valid (not expired, correct issuer etc...),\nUse the JWT (read /groups from the claims/payload for example).\nThis list immediately shows two really impacting points:\nOn the overall cluster we will pay the token validation again and again on each service (instead of paying 1 we will pay 4 in the simplified example we had before),\nEach service must have access to the validation data (valid issuer, signature key/certificate, time window validation - there is often some tolerance for expiry case).\nThe first point implies a high cloud/machine cost and the last one a high cost in terms of maintenance/deployment/ops.\nThe simplest solution to solve this cost is to ensure it is paid only once. This is done adding/relying on the gateway in front of all services:\nWith this new architecture, we only validate read the JWT once....but we loose the JWT claims - which is the only interesting part functionally. For some applications it will be ok-ish (for applications only requiring to be logged in) but generally it will not. This is where the gateway will add a real value because its roles now become:\nto validate the JWT and ensure the call is valid,\nto explode the JWT payload and propagate needed informations through headers.\nOnce this second step is done, the calls now look like:\nWith such solution, the important JWT information - here the sub which is the username/login and groups which contains the list of roles - are propagated through dedicated headers to services.\nthe JWT- prefix is a convention but you can use any header name, just ensure to normalize it in the company to enable a service to transparently forward them if needed by filtering them on the prefix.\nThe last step you can go through is to validate the role on the gateway when possible but often it will be linked to some business rules so will need to be done in the service.\nMeecrogate gateway enables to implement this pattern by configuration. Using the JSON configuration - but it works exactly the same with the environment variables configuration, the route will be composed of 4 sections:\nThe matcher section which specifies how to select this route configuration for an incoming request,\nThe proxy section which specifies where to forward the request once validated/prepared,\nThe shield section which specifies how to validate the request before starting to prepare it and forwarding it,\nThe enricher section which enables to modify the incoming request before forwarding it to another service.\nAssuming we are defining the route for the service 1 of previous diagram, the matcher section can look like:\nSimilarly, the proxy section will just target the service1 host:\nNow the core of the security (the shield layer) will validate our incoming JWT:\nNow our JWT is validated, we can modify the incoming request to forward the needed information to the actual service:\nIf you migrate from a JWT parsing solution (microprofile-jwt-auth, spring-boot-starter-security, etc...) to this header based solution you will still need to load the security context but you can now trust the incoming headers since the JWT was validated properly and information are properly forwarded.\nIn terms of (pseudo-)code, it means moving from this kind of implementation:\nto\nThe interesting part is that the change is mainly about the part before the comment line. This means that once integrated with your security framework (from a simple interceptor to the full security framework like Apache Shiro or the widely spread Spring security), it does not change your application much.\nthis is one of the reason to take time to properly setup your application stack and not take the shortcut to code transversal concerns like security ones in the business layer.\nNow the JWT extracton is not that complex to implement, in particular since the JVM or most common languages contain all the needed bricks, but its validation can be tricky and if you do an error there, there is no more security in the system. On another side, the user context loading is only about reading headers and optionally parsing a json value for most complex cases so it is very very doable by any framework and developer.\nTo illustrate that I will just show what extractUserContext can look like in a JAX-RS based application in two steps:\nDefine the UserContext model,\nInject the user context instance in your endpoints.\nHere is how tp define the user context we spoke about ealier:\nOnce we have an user context we can simply inject it into any endpoint:\nmaking UserContext hosting some security helper method makes it generally easier to use.\nIt is natural to think that any service can play that gateway role and that you can keep microprofile-jwt-auth, spring-security etc to do that gateway role. Technically it is true but the key point here is the architecture: the service should be central (which does not mean a SPoF, previous Meecrogate example has no SPoF since it scales properly horizontally for example). Using a service, the risk - which is quite high - is to start adding other services bypassing it and being back to the original issue so the rule of thumb there is to use a gateway. Techically you can use whatever you want from a plain HTTPd/NGINX/Tomcat with custom glue code to a real gateway like Meecrogate one but ensuring the instances act as gateways is what will make this solution robust in time.\nAnother good reason to make some instances acting as a gateway is to enable them to use a cache. Once some instances are the \"gateway\", you can optimize them to have the relevant CPU and memory profile making your security layer very cheap for end user in terms of experience which is key to offer a good service.\nDepending the size of your company you can also be concerned about security audits, and in such a case, having a gateway enables to justify only once about the security concerns whereas doing it in all services will require to provide such guarantees in all services which can be a lot of efforts and as much errors you can encounter.\nLast point is that once you passed the gateway, everything is \"user context\" which means it will integrate smoothly with messaging system needing the user context and will not require Kafka topics to be replayed overloading the authentication system for example (using CQRS pattern it happens very easily when global architecture was not thought upfront).\nWe saw in this post that:\ncloud and microservice deployments require some understanding of the overall system and not just of each piece compsing it alone,\nusing JWT token enables stateless deployments without having to imply an outstanding CPU/resource consumption,\nusing a gateway enables to focus on the business in services,\nusing a gateway enables concentrate the security effort and user context modelling in a single location,\nusing a gateway enables to optimize resource consumption,\nusing a gateway enables to have better guarantees about the security and simplify the audits.\nSo the conclusion of this post is: always think your system widely and not per service and if relevant use a real gateway!","title":"Reduce CPU crypto load and deployment maintenance thanks to Meecrogate Gateway","url":"/blog/reduce-cryto-cpu-load-and-maintenance-on-your-services-thanks-meecrogate-gateway.html"},{"lang":"en","lvl2":"Context\nTypescript Example","lvl3":"login component\nretrieve the token\nConclusion","text":"This blog post describes a front end implementation of the password flox through the meecrogate oauth2 server. We consider that the backend is read and that the oauth2 server is available. We will be focusing on how to implement the front end application with React and material to make it a bit more fancy. The first step is to register our client on the Oauth2 server with a password grant type. The Oauth2 server offers few flow options:\nauthorization code\nPKCE\nclient credentials\npassword\nrefresh token\nYou can find more information about those different flows in here^ .\nFor this example we will register our client with 'Password' type, so from the front end we will have the following steps to manage:\n. go to the login page of the react application . login . retrieve the access_token\nThis example has been implemented with React so it is in typescript and can be adapted for angular or other frameworks.\nWe will not describe the whole react application but the login component will be enough.\nThe code above is the whole component code. We will focus on the call to the oauth2 server in the next section.\nOn our react application we have a method that will call the oauth2 server with the username/password provided by the user. The response will either contain the access token or return the error message.\nif the response contains the access_token we can forward the user to our application and use this token to contact protected apis and display the information. If the response does not contain the token but does contain an 'error_description' field we can then display this information to tell the user what problem he is facing.\nIn few steps we have been able to implement a simple password login flow for our web application with the Meecrogate Oauth2 Server. In this example we have been focusing on the simple password flow but you can check the other blog posts about more complex flows.","title":"Simple user/password authentication in your web application with the Meecrogate OAuth2 server","url":"/blog/password-flow-frontend-application.html"},{"lang":"en","lvl2":"What is a Batch?\nWhat are batch challenges\nConfiguration\nYupiik Batch\nGoing further\nConclusion","lvl3":"Monitoring\nYupiik Batch DSL\nYupiik Batch Base Components\nYupiik Batch UI","text":", Romain Manni-Bucau , 2022-11-15, 8 min and 32 sec read\nTo understand where Yupiik Batch comes from, we must step back and review what we call a batch.\nIn IT, we commonly differentiate two main application types:\nLong running applications: applications which start and are not supposed to stop. It is the case of web application where the server is supposed to serve any request any time or daemon applications which are running all the time (for example the network manager on your computer).\nTask oriented applications: it concerns applications starting and stopping once their task is done. It concerns the simplest command line interface (date command for example which just prints the current date) to the longest batch which can take hours but once it completed its task, it exits.\nIndeed, when we speak about batche, we speak about the second category. What will make this \"task\" oriented application a batch is the philosophy of the development which is generally to handle a huge data volume - but not yet a big data one - and try to optimize the processing of these data.\nConcretely, and to illustrate that last statement, if we insert rows in a table, we'll insert them with a bulk logic and not one by one.\nSo writing a batch is respecting a philosophy more than a framework, so why are there so framework out there?\nIt is because batches bring some constraints and the interesting thing is that it is not in the development itself but in the monitoring/administration of the batches.\nWhen you have a web application, you monitor it with its logs and some endpoints (health check, metrics, ...). For a batch you have logs and that is it. If you want to check what happent it can be tricky and you will need to do some advanced querying on your logs.\nWhile it stays a common requirement, batch frameworks also often bring a way to track what is executed and store that (often in a database) to then expose an UI on these data and let you review what was done.\nThe simplest solution is a row per execution of batch (a.k.a. job execution) and generally it is linked to a list of step executions which are all the steps the batch did. For example \"read data from the web service\" or \"write data to this database\".\nThis is exactly what Yupiik-Batch will bring with as minimal requirement as possible.\nAnother common need of batch application is to be configurable.\nIndeed there are plenty of solutions there but you must pick a solution which is flexible enough to:\nBe injectable in the application (i.e. be configurable from the environment and/or the system properties of the java command),\nBe documentable since the first users of the configuration will be the ops guys,\n(optionally) Be easy to integrate. One example of that is to easily generate a form from your configuration to re-run a batch for example if needed.\nYupiik Batch design is to be simple and as close as possible to plain Java code.\nThere is no constraints on the way you write the batch itself but the recommended way is to:\ndefine your configuration using simple-configuration which consists in marking a Java class fields with @Param:\ndefine a batch implementing Batch<YourConf>\nimplement the batch in accept method - where the configuration instance is considered initialized and injected. Here it is recommended to use the Batch DSL:\nlaunch the batch using Batch.run which will automatically populate a configuration instance from the environment and call accept method of your batch:\nThe Yupiik Batch DSL is very simple and close to Stream API since it exposes map, filter, then (equivalent to accept in Stream API).\nIt always takes as first parameter a name (of the step) and second parameter a functional interface matching the method name (Function, Consumer, Predicate).\nThis interface has CommentifiableX - for example CommentifiableFunction - flavors which enable to attach to the step a comment which is tracked as well (so when you check what was executed you get the step name, status, duration + a custom comment enabling an easier understanding of the execution and potentially failure reason).\nA very important feature of using simple base API like Function is to ease to write components and combine them in higher level components (HLC). It enables to create higher level features but also to fully control the reporting (steps) level you want.\nkeep in mind the reporting is the way you communicate between the dev and ops \"layers\" of the project.\nYupiik Batch decided to not provide trivial components so you will not find a generic http client data reader for example - this, generally, requires too much toggles to be usable and a custom one is simpler to maintain.\nHowever, some components are provided encapsulating common logic. You can find them listed on Yupiik Batch Components documentation.\nWithout entering into the details here are some very interesting highlights:\nDatasetDiffComputer which computes a Diff between two sorted Iterator of data. It is very convenient and efficient to synchronize two datasets (a REST API with a database or two databases for example). It comes with its DiffExecutor companion which enables to execute a Diff and apply it on an output storage (you provide the implementation of the actual storage so it can be a REST API or plain SQL implementations).\nMapper enables to use an annotation driven mapping between two types (records):\nThe key feature there is to fully describe the mapping between two records statically. This enables to use MapperDocGenerator class to generate the documentation of the mapping and let documentation readers to know what is done in the mapping step (ops for example).\nExecutionTracer which, once configured in the RunConfiguration passed to the run method of the Yupiik Batch DSL, enables to store the job and step executions.\nThis UI reads jobs and steps from a database. It is generally the UI associated to the ExecutionTracer tracker.\nHere is what it can look like:\nThen clicking on a batch identifier you can see the steps the batch executed:\nthe small + buttons enables to see the comment associated to the line it is shown on.\nThe last note to mention about the UI is that it has js extensions. I will not enter into the details in this post but high level it will enable you to:\nAdd routes to the UI - so add features like grabbing logs, metrics, etc...\nReformat/rewrite the rows so you can rewrite comments to insert direct links to some other application or data,\nEnable you to directly access the data the batch works on,\nEtc...\nWe saw that Yupiik Batch provides:\nA simple deployment friendly configuration solution well integrated with the batch runtime,\nA simple to deploy and wire UI to monitor your batches,\nA simple and very flexible programming model Java friendly to write any batch application.\nThis is what is code focused, but if we step back we have to consider a few more points:\nIt integrates very well in Kubernetes through Job or CronJob and the configuration make it very smooth to use with a dedicated ConfigMap (highly recommended).\nFor a complete E2E deployment you can integrate it very easily with Yupiik Logging which will enable you to have a JSON logging and therefore make the Kubernetes log aggregation way more efficient. The tip there is to extend the default JSON formatter to inject a batch identifier which will enable you to filter logs per execution too (and potentially wire it in the comment automatically).\nIt is easy to integrate which enables to generate and deploy a static documentation on Github/Gitlab/FTP pages very easily.\nYupiik Batch was used to deploy hundreds of batches and with some good successes. Batch UI enabled us to monitor the batches very efficiently and the programming model enabled us to integrate in any environment very easily. The very nice point is that the integration in Kubernetes is super smooth and it avoids a lot of other code or infrastructure, even for cron jobs which are now supported natively by Kubernetes/OpenShift.\nSo stay simple is likely what applies more than ever within the cloud erea and it is exactly what Yupiik Batch was built for.\nFrom the same author:\n\nRomain Manni-Bucau\nIn the same category:\nTechnology\nPrevious\nAll posts\nNext","title":"Yupiik Batch Introduction","url":"/blog/yupiik-batch-introduction.html"},{"lang":"en","lvl2":"What's new?\nHow to start?\nFeedback are welcome","text":", Francois Papon , 2022-12-27, 41 sec read\nThis releases focuses on making it easier to customize placeholders handling and extraction (doc).\nThis release include fixes and improvements, more details are available here:\nThe documentation is up-to-date here:\nYou can start be reading the Getting Started page .\nOn Linux amd64 computers, you can install the CLI by executing this command in your shell:\nYou can contribute, report bug or ask for improvement in the GitHub issue page\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Bundlebee 1.0.19 Released","url":"/blog/release-yupiik-bundlebee-1.0.19.html"},{"lang":"en","lvl2":"What's new?\nHow to start?\nFeedback are welcome","text":", Francois Papon , 2023-06-13, 50 sec read\nThis releases focuses on:\nMore placeholders for default observability stack\nNamespace placeholder keyword support to enable to reuse globally configured namespace in placeholders\nProper DaemonSet usage for loki\nSupport overriding mechanism for properties import for placeholders (instead of failing on conflicts)\nThis release include fixes and improvements, more details are available here:\nThe documentation is up-to-date here:\nYou can start be reading the Getting Started page .\nOn Linux amd64 computers, you can install the CLI by executing this command in your shell:\nYou can contribute, report bug or ask for improvement in the GitHub issue page\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Bundlebee 1.0.20 Released","url":"/blog/release-yupiik-bundlebee-1.0.20.html"},{"lang":"en","lvl2":"What's new?\nHow to start?\nFeedback are welcome","text":", Francois Papon , 2023-09-08, 53 sec read\nThis releases focuses on:\n[escaping][filtering] rework filtering to not break any escaping but only handle mustache like ones.\n[process] Enhance process command to support the collection of descriptors in a single file - avoids to mess up dump when templates are used.\n[build] Fix native execution on Lint helper.\n[dependency] Upgrades.\nThis release include fixes and improvements, more details are available here:\nThe documentation is up-to-date here:\nYou can start be reading the Getting Started page .\nOn Linux amd64 computers, you can install the CLI by executing this command in your shell:\nYou can contribute, report bug or ask for improvement in the GitHub issue page\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Bundlebee 1.0.21 Released","url":"/blog/release-yupiik-bundlebee-1.0.21.html"},{"lang":"en","lvl2":"What's new?\nHow to start?\nFeedback are welcome","text":", Francois Papon , 2023-06-07, 34 sec read\nSupport injection parameters on @OnEvent methods\nSmoother k8s client\nBetter OpenTelemetry/Zipkin integration\nThis release include fixes and improvements, more details are available here:\nThe documentation is up-to-date here:\nYou can start be reading the Getting Started page .\nYou can contribute, report bug or ask for improvement in the GitHub issue page\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Fusion 1.0.3 Released","url":"/blog/release-yupiik-fusion-1.0.3.html"},{"lang":"en","lvl2":"What's new?\nHow to start?\nFeedback are welcome","text":", Francois Papon , 2023-06-28, 51 sec read\nNew module fusion-jwt to parse and validate a JWT just using fusion-json dependency and the JVM\nImprove datasource rollback on sqlexception errors\nMany improvement in the fusion-handlebars templating module\nAdd Body API in fusion-http-server module to easily manage request body usage through the java api Flow\nUpgrade to Apache Tomcat 10.1.10\nMake the configuration of the K8S http client more easy\nTracing and logging improvement\nThis release include fixes and improvements, more details are available here:\nThe documentation is up-to-date here:\nYou can start be reading the Getting Started page .\nYou can contribute, report bug or ask for improvement in the GitHub issue page\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Fusion 1.0.4 Released","url":"/blog/release-yupiik-fusion-1.0.4.html"},{"lang":"en","lvl2":"What's new?\nHow to start?\nFeedback are welcome","text":", Francois Papon , 2023-07-19, 55 sec read\n[persistence] adding contextless Database (explicit connection parameter).\n[jwt] support list in Jwt.claim().\n[processor] handle optional imports for generated modules.\n[graalvm] generate resources.json and native-image.properties to include fusion json metadata (openrpc endpoint).\n[http-server] Enable to deploy a monitoring server by configuration, ensure request is completed even if completionstage does not return but fails.\n[jsonrpc] jsonrpc uses List and not array for serialization.\n[json] better enum support and jsonschema integration, tolerate to pass Map as a synonym of Object for generic mapping.\nThis release include fixes and improvements, more details are available here:\nThe documentation is up-to-date here:\nYou can start be reading the Getting Started page .\nYou can contribute, report bug or ask for improvement in the GitHub issue page\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Fusion 1.0.5 Released","url":"/blog/release-yupiik-fusion-1.0.5.html"},{"lang":"en","lvl2":"What's new?\nHow to start?\nFeedback are welcome","text":", Francois Papon , 2023-08-22, 48 sec read\n[persistence] tolerate nested table as embeddable for cases with more than 255 columns.\n[jsonrpc] adding PartialResponse support to customize headers on jsonrpc responses.\n[jsonrpc] Tolerate OffsetDateTime, ZoneOffset and LocalDate as root parameter on a JSON-RPC endpoint.\n[http-server] Enable parameter access from body - to avoid to parse it.\n[processor] Improve incremental compilation.\n[dependencies] Upgrade Tomcat to v10.1.12.\nThis release include fixes and improvements, more details are available here:\nThe documentation is up-to-date here:\nYou can start be reading the Getting Started page .\nYou can contribute, report bug or ask for improvement in the GitHub issue page\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Fusion 1.0.6 Released","url":"/blog/release-yupiik-fusion-1.0.6.html"},{"lang":"en","lvl2":"What's new?\nHow to start?\nFeedback are welcome","text":", Francois Papon , 2023-09-26, 49 sec read\n[dependencies] Upgrade to Apache Tomcat 10.1.13.\n[httpclient] basic rate limiting support.\n[subclassing] tolerate class templates/generics at some point.\n[k8s-operator] adding kubernetes-operator-base module.\n[kubernetes operator base] handle bookmark events.\n[zipkin] Ensure span tags are only strings since it would require a mapping step otherwise.\n[handlebars] better item template handling for each loop.\n[processor] Add support for Enum fields.\nThis release include fixes and improvements, more details are available here:\nThe documentation is up-to-date here:\nYou can start be reading the Getting Started page .\nYou can contribute, report bug or ask for improvement in the GitHub issue page\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Fusion 1.0.8 Released","url":"/blog/release-yupiik-fusion-1.0.8.html"},{"lang":"en","lvl2":"IoC\nSpring/Spring-Boot\n3rd party beans\nCDI\nFusion\nConclusion","lvl3":"Injections\nScopes","text":", Romain Manni-Bucau , 2023-04-20, 6 min and 54 sec read\nThe Inversion of Control (IoC) is the simple concept to link objects between them from contracts and not require you to do the plumbing. Concretely you write My billing service needs a customer service and not My billing service uses the database customer service.\nThe impact? You can change the behavior of the services consuming your service as a dependency by plugging another implementation. In previous example, if you change of customer service to move to elasticsearch, the billing service still works.\nConcretely in Java there are two main ways to do that:\nA declarative way which is the most known solution (@Inject and friends),\nThe programmatic way (lookup and derivatives).\nThe counter part is to be able to define beans. Most of the time it is about declaring a class (potentially implementing the contract as an interface if the contract is not the class itself). But for 3rd parties, you need to be able to define a method as a factory of bean instances since you don't control the class and don't always want to wrap/decorate the 3rd parties to still integrate with the ecosystem of these libraries.\nLastly, a common piece of an IoC and Dependency Injection framework is to handle scopes, i.e. define when to create/destroy an instance. Common cases are per application (singleton), per request (take care it is NOT the servlet request in general), per session or per injection (each injection has its own instance).\nin this post we will not discuss the qualifier notion which enables to remove the ambiguity between two beans of the same type.\nSpring-Boot is likely the oldest and most used IoC framework in Java. it is backed by Pivotal and designed to be reflection/runtime (even if the recent GraalVM track enables to get AOT working, code stays runtime oriented).\nIn spring ecosystem, the injection can be handled with four main solutions:\n@Autowired: the historical and default way to mark a field as injected,\n@Inject: alias for @Autowired coming from the JSR330 (pseudo standard of injections but without defining much behavior actually),\nConstructor injections, i.e. the constructor gets parameters injected from its signature simulating all parameters have @Autowired,\nProgrammatic lookups using the ApplicationContext or BeanFactory depending your case.\nHere are the most common examples:\nTo define a bean of a type you don't declare you can use @Bean (generally in a @Configuration class):\nScopes are mainly a way to request injections to be replaced by a proxy which will delegate to an instance depending of the context (ex: request).\nIt is mainly a matter of marking a bean (class or method) with a scope annotation or the generic @Scope annotation. Then the lookup of the actual instance will be delegated to some plumbing between the annotation and proxy.\nHere is how to declare a bean request scope:\nCDI is the standard inherited from JavaEE and now hosted at Eclipse under JakartaEE organisation. It intends to provide a portable and normalized way to do IoC and stands for Context and Dependency Injection.\nIt has three main runtimes:\nlight: which targets mainly GraalVM to be explicit,\nfull: which targets standalone applications or more generally any CDI application. This one inherit from light but has a runtime Extension mecanism to modify the application at startup. As of today it overlaps a lot with the light API which got added in last release but only this API defines when it is executed so it is more stable than light which is undefined as of today.\nEE: integration in EE containers, it is mainly full flavor without the standalone container API.\nIn CDI ecosystem, the injection can be handled with two main solutions:\n@Inject: similar to the spring @Autowired, it can be set on a field, constructor (to select the constructor to use) or a setter.\nProgrammatic lookups using the BeanManager or Instance.\nTo define a bean of a type you don't declare you can use @Produces in any bean, note that comparing to spring you must define a @Disposes method if you need to cleanup the bean:\nIn CDI, scopes are very similar to Spring. To define a request scope instance, just mark it as such for example:\nCDI has the notion of normal scopes (an instance is always the same in a single context) which require the beans to be proxyable (class non final with a no-arg constructor etc...).\nYupiik Fusion is very similar even if it intends to be very lightweight so you shouldn't be lost. The key difference is that it is closer to dagger by generating the plumbing of the application at build time but keeping the capacity to reconfigure the wiring at runtime using modules.\nIn Fusion ecosystem, the injection can be handled with two main solutions:\n@Injection: similar to the spring @Autowired, it can be set on a not private field.\nProgrammatic lookups using the RuntimeContainer.\nTo define a bean of a type you don't declare you can use @Bean in any bean, if the instance implements AutoCloseable it is automatically alled to destroy the instance:\nIn Fusion scopes are annotations:\nas of today, Fusion only handles @ApplicationScoped to define singletons and @DefaultScoped to mark a bean as being created per injection/lookup. Note that the application scope behaves close to CDI one by using a proxy and lazy instantiation of the underlying bean whereas Spring will use eager instantiation.\nThis post is a high level overview of three Java IoC. A lot more is interesting to compare like the facts:\nFusion generates all the code at build time and limits the runtime to the lookup resolution,\nthe resolution itself which is not the same for all three frameworks,\nthe Fusion ecosystem which is not only an IoC,\nthe way to tune the IoC when it starts with extensions/modules,\nthe integration more or less smooth with GraalVM to become native,\nand much more!\nTo learn more, you can check the online documentation and the source code repository .\nEnjoy!\nFrom the same author:\n\nRomain Manni-Bucau\nIn the same category:\nTechnology\nPrevious\nAll posts\nNext","title":"Yupiik Fusion Injections","url":"/blog/fusion-injections.html"},{"lang":"en","lvl2":"Why Yupiik Fusion?\nWhat is Yupiik Fusion?\nGet started\nExample with JsonRPC endpoint\nGraalVM friendly\nConclusion","text":", Francois Papon , 2023-04-05, 4 min and 7 sec read\nSince about 2021 all JavaEE specifications moved from javax to jakarta packages which broke basically the full Java ecosystem. In the mean time, the industry became more cloud and Kubernetes focus which triggers new challenges like being native/GraalVM friendly. From the two strong statements, being vendor agnostic and light, Yupiik created Fusion !\nAt Yupiik, we created an api platform and decided to open source the kernel of our product and share it with the community because we are heavily involved in open source projects.\nYupiik Fusion is a very light weight and powerful framework to build cloud native microservices. It means that you have all your need to build the most common use cases in an efficient way by using your favorite programming language (Java).\nFor cloud applications, being the most reactive possible is a key criteria so Fusion chose to:\nBe build time oriented : only delegate to runtime the bean resolution (to enable dynamic module aggregation) and not the bean and model discovery nor proxy generation,\nStay flexible : even if the model is generated at build time you can generally still customize it by removing a bean and adding your own one,\nBe native friendly : some applications need to be native to start very fast and bypass the classloading and a bunch of JVM init, for that purpose we ensure the framework is GraalVM friendly - using Apache Geronimo Arthur or not.\nThe framework come with the most common features and extension such as:\nReflection less IoC processor,\nEvent bus to help beans to communicate between them,\nHTTP server which is an abstraction of the Apache Tomcat web server,\nHTTP client based on the java.net.httpClient with an extended configuration,\nJsonRPC endpoint server,\nJson support mapper for serialization/deserialization,\nCLI helper to easily build command line application,\nConfiguration model with environment variables abstraction mapping designed for the cloud,\nFull Java Record integration,\nSimple Persistence layer based on common jdbc prepare statement and Record binding,\nObservability module to publish logs, metrics, healthchecks and tracing,\nHandlebar templating helper,\nJUnit5 support for writing tests,\nGraalVM friendly and easy build\nThe magic of Fusion is on the processor part that has the responsibility to build the runtime, so you just need to depend on the fusion-api in compile scope (which is the default with Apache Maven) to be included to the application.\nyou can learn more on the Setup page.\nNote the provided scope which enables to bring automatically the build time generator without having to deliver it at runtime so your deliverable stays light!\nThat's all!\nTo implement a simple JsonRPC endpoint, you just need to add these dependencies into your pom.xml:\nthe fusion-jsonrpc module depend on the fusion-http-server and the fusion-json, so you can configure and use them without adding them to the dependencies\nBuild the application:\nThen add the Apache Maven exec plugin to test the execution:\nwe are using the ready to use Fusion launcher but user can create a custom main method to start the application container .\nOnce executed, you can call your endpoint:\nThanks to JSON-RPC, if you have multiple customer to create in one time, you can use the bulk option:\nThanks to Apache Geronimo Arthur project, it's very easy to build your application in native just by adding the maven plugin dependency:\nThen you can build the binary of your application:\nOr build a docker image with the binary include:\nSorry to disappoint you if you are thinking that it will be hard to build a native app!\nThis post is the announcement of the new Yupiik Fusion project with a short startup presentation, and we will add new blog posts about another examples to show how to use the full power Fusion.\nTo learn more, you can check the online documentation and the source code repository .\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nTechnology\nPrevious\nAll posts\nNext","title":"Yupiik Fusion Introduction","url":"/blog/fusion-introduction.html"},{"lang":"en","lvl2":"Reminder\nJDBC and ThreadLocal\nJava 8 was a game changer\nFusion ThreadLocal-less Database\nConclusion","lvl3":"Virtual Threads","text":", Romain Manni-Bucau , 2023-07-30, 4 min and 24 sec read\nJava is welcoming virtual threads - coroutines in most languages - but due to technical limitations, ThreadLocal will be too costly for them even if they work. To solve that, Java is creating structured concurrency but is it always the best solution?\nThreadLocal are a way to store a data in the context of the thread and get it back in a nested call even if the value was not passed as a parameter. A common example is to get the request:\nAs you can see, the HTTP request is not a parameter but the value is read from the method. Technically it is often set from a servlet Filter or equivalent.\nThe issue with virtual threads is that you can get way more threads than with common threads - indeed since they are not threads but tasks in a shared executor service, it is literally like using yourself an executor service with tasks which can be suspended/restored so memory is the limit instead of the OS number of threads.\nThreadLocal being a bit costly so java created ScopedValue. The underlying idea is close but the API is different:\nIn this example, myMethod can call REQUEST.get(), similarly to ThreadLocal code.\nwe can envision a future JDK where ThreadLocal are backed with limitations to ScopedValues a bit like java.io got reimplemented with java.nio recently to make virtual threads more widely usable and efficient as in other languages.\nStrictly speaking there is no ThreadLocal in JDBC but they are used in most applications due to transaction management. A good example is the Spring `DataSourceTransactionManager` - or any other JTA/JakartaEE based manager. It uses a ConnectionHolder which is bound in a TransactionSynchronizationManager resource which ends in a `TransactionContextHolder` which is just a wrapper for a ThreadLocal.\nHigh level, the need is to be able to wrap multiple calls and use the same transactions, however deep the calls are.\nJava always supported message passing pattern but Java 8 with lambdas, and a lighter syntax, enables to make it a first citizen of our code.\nConcretely, it is very common now to replace interceptors with a method wrapper taking a lambda.\nFor example to make a method transactional, we went from:\nto\nAs a quick reminder, Fusion persistence module is a light mapper on top of JDBC. You can see it as a light JPA without any relationship support, just bind ResultSet to Java records or the records to statements. This feature is encapsulated in a class called Database.\nThe first implementation inherited from our dozens of years of experience in JDBC support and reused the ThreadLocal API but since it was lambda based we were very easily able to modify the pattern to add to most methods the JDBC Connection.\nSo concretely we went from:\nAs it was evangelized something like 8 years ago when Akka or reactive programming popped up, message passing - fact to use method parameters - enables to always have the context to use in the call.\nThanks to lambdas, it becomes easier to do since you can orchestrate calls more easily.\nThe impact is to add the context to methods, or said otherwise, ensure they have all their dependencies as parameters.\nUsing these patterns we now have in Fusion persistence method wrapping DataSource.getConnection and Connection.close in read/write mode. We wrapped it in a TransactionManager but this one does ont use any ThreadLocal, just plain lambdas:\nNow it becomes quite easy to pass the connection to the JDBC wrapping methods:\nWith that, if your JDBC driver does not pin threads (does not use synchronized for example), it will be virtual thread friendly and optimize the throughput of your application.\nThis post shows that, even if technically you can always implement complex solutions, getting back to the basics and the generic patterns adapted to your coding style enables to keep things simple and efficient.\nAs a generic mindset, it is always preferrable to use message passing when possible and really when not desired wrap it with ScopedValue if you know you will run in virtual threads or in ThreadLocal if in generic threads - see how things become more complex if you write a generic lib ;).\nTo learn more, you can check the online documentation and the source code repository .\nEnjoy!\nFrom the same author:\n\nRomain Manni-Bucau\nIn the same category:\nTechnology\nPrevious\nAll posts\nNext","title":"Yupiik Fusion and ThreadLocal-less database","url":"/blog/fusion-theadlocalless-database.html"},{"lang":"en","lvl2":"What's new?\nHow to use?\nFeedback are welcome","text":", Francois Papon , 2023-03-07, 30 sec read\nThe docker image is based on Zulu Java 17.0.6 JDK\nYou can install the image in your local docker registry:\nYou can contribute, report bug or ask for improvement in the GitHub issue page\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Java 17.0.6 Docker Image Released","url":"/blog/release-yupiik-java-17.0.6.html"},{"lang":"en","lvl2":"What's new?\nHow to use?\nFeedback are welcome","text":", Francois Papon , 2023-03-17, 30 sec read\nThe docker image is based on Zulu Java 17.0.6 JDK\nYou can install the image in your local docker registry:\nYou can contribute, report bug or ask for improvement in the GitHub issue page\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Java 17.0.6.1 Docker Image Released","url":"/blog/release-yupiik-java-17.0.6.1.html"},{"lang":"en","lvl2":"What's new?\nHow to use?\nFeedback are welcome","text":", Francois Papon , 2023-05-09, 34 sec read\nThe docker image is based on Zulu Java 17.0.7 JDK\nYou can install the image in your local docker registry:\nor use it as dependency to build docker image:\nYou can contribute, report bug or ask for improvement in the GitHub issue page\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Java 17.0.7 Docker Image Released","url":"/blog/release-yupiik-java-17.0.7.html"},{"lang":"en","lvl2":"What's new?\nHow to start?\nFeedback are welcome","text":", Francois Papon , 2023-03-07, 40 sec read\nNo more need of JSON-P dependency for json logging\nEnables to pass a function taking the log record and converting it to a map of data to append to the json object\nThis release include fixes and improvements, more details are available here:\nThe documentation is up-to-date here:\nYou can start be reading the Overview page .\nYou can contribute, report bug or ask for improvement in the GitHub issue page\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Logging 1.0.7 Released","url":"/blog/release-yupiik-logging-1.0.7.html"},{"lang":"en","lvl2":"Yupiik Open Source Stack","lvl3":"Projects\nBlog\nCommunity","text":"From microservices to cloud deployment, we provide a full stack to simplify all the aspect of development focusing on productivity and efficiency.\nList of the Yupiik stack projects.\nWe love Open Source Software, and we are happy to share some of our work. Learn more\nLatest news\nThe latest news and tutorials about technology. Learn more\nCommitted contributor and member\nPart of our team are the developers and contributors of many well known Open Source projects from world wide organizations. Learn more","title":"Yupiik OSS","url":"/index.html"},{"lang":"en","lvl2":"Projects","text":"Yupiik open-sourced a set of tools targeting application development and deployment. It aims at simplifying the all the aspect of development focusing on productivity and efficiency.\nBundleBee is a light Java package manager for Kubernetes applications.\nReflectionless and selfcontained framework (no jakarta/spring/guice dependency).\nAn Apache Maven plugin set of tools (living website, PDF adoc renderer, HTML5 slides, Github Release synchronizer).\nLightweight microservice stack for modern applications.\nSimple JSON/XML2JSON CLI formatter.\nBatch runtime framework.\nLogging Extensions (JUL + GraalVM integration).","title":"Yupiik OSS Projects","url":"/projects.html"},{"lang":"en","lvl2":"Bundlebee\nYupiik-Tools-Maven-Plugin\nYupiik-Batch\nFusion","lvl3":"bundlebee-1.0.24\nbundlebee-1.0.25\nDocumentation\nHow to start?\nFeedback are welcome\nyupiik-tools-maven-plugin-parent-1.2.0\nyupiik-batch-1.0.4\nyupiik-batch-1.0.5\nfusion-1.0.13","text":", Francois Papon , 2024-01-10, 1 min and 54 sec read\nSome great improvements has been done, especially:\nArgoCD basic integration (as an alternative to helm/kustomize)\nDiff command support (between alveolus and Kubernetes state)\nSimple ciphering support in placeholders\nHTTP proxy configuration support\nHighlight: one more step to demo that BundleBee is GitOps friendly (but not limited to) and automation friendly.\nURL: https://github.com/yupiik/bundlebee/releases/tag/bundlebee-1.0.24\nPublished at: 08/01/2024\nURL: https://github.com/yupiik/bundlebee/releases/tag/bundlebee-1.0.25\nThe documentation is up-to-date here:\nYou can start be reading the Getting Started page .\nYou can contribute, report bug or ask for improvement in the GitHub issue page .\nNew great improvements around asciidoc integration:\nImplement a java native asciidoc parser + renderer\nImplement ascii2svg in plain java for git friendly diagrams\nAdd copy button to code snippets in Minisite\nURL: https://github.com/yupiik/tools-maven-plugin/releases/tag/yupiik-tools-maven-plugin-parent-1.2.0\nRelease 1.2.0\nThe main improvement is the new Metric Relay module link .\nURL: https://github.com/yupiik/yupiik-batch/releases/tag/yupiik-batch-1.0.4\nPublished at: 07/12/2023\nURL: https://github.com/yupiik/yupiik-batch/releases/tag/yupiik-batch-1.0.5\nPublished at: 21/12/2023\nThe Fusion project continue to grow and the latest major improvements are related to:\n[living doc] openrpc 2 asciidoc support\n[httpclient] RoutingHttpClient to switch between different client configuration based on a custom rule\n[doc] explain how to avoid warnings with Java >= 21\nURL: https://github.com/yupiik/fusion/releases/tag/fusion-1.0.13\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Releases Roundup","url":"/blog/releases-blog-post-202401.html"},{"lang":"en","lvl2":"The challenge\nThe stack\nRational of the stack\nGet started","text":", Romain Manni-Bucau , 2023-01-24, 4 min and 45 sec read\nToday's the backend development evolved enough to join the frontend in terms of velocity requirement. It means that you should be able to develop and deploy a backend services in a few days and not weeks or months.\nAnother challenge is the fact the consumers multiply the requests done to the backends. This means that the resources consumption is rarely optimised and it often adds a lot of latency (if you use REST model for example).\nOn another side, the cloud showed that security is a key requirement of any application.\nTo solve these challenges, it is important to simplify the development stack to ensure:\nYou control it\nAny developer can work with it very quickly - simplicity is often more important than other criteria for the velocity to stay high in time\nBUT you can still optimize and improve it when needed for very high throughput applications for example\nTo answer modern needs, Yupiik developed these bricks:\nAnd the cherry on the cake is that it is GraalVM friendly so you can make your application native and logging configured at runtime!\nKeeps the configuration light and documentation friendly\nJSON-RPC enables to develop \"commands\" and optionally to bulk them, this stack allows to optimize bulk requests very easily with any custom push-down logic (execution plan)\nSince it is command oriented, it is compatible with CLI application (reuse, reuse, reuse) and messaging (JMS, Kafka) applications.\nIt allows to test locally very easily with minikube, microk8s, ... but also to deploy to production directly with the same recipe!\nIt is generation friendly - so you can generate the configuration documentation or API documentation from plain code. Living documentation - the fact to generate all you can to ensure it is up to date - is a key target of this module/\nIt is also github pages or gitlab pages friendly and handles the deployment for you.\nReact/Preact are light, way less complex, unstable and are more javascript friendly than recent angular or Vue.JS versions (even if this last one tend to copy others version after version)\nEsbuild is insanely fast, easy to integrate with a backend mock if needed an totally defeats webpack as of today when you start from scratch.\nFinally, even if all the stack works with any Java compatible base OCI image, we also provide our own Azul image based on alpine and a bit lighter than default one.\nThe application flow is as follow:\nThe application is developed with Apache Maven and Java using Yupiik Stack\nThe CI takes any incoming changes and validate them (with tests for the application code but also the deployment)\nOnce validated, the changes are snapshoted in a binary (artifact + OCI/docker image) pushed on CI repositories\nIt can be dev environment + Github Pages (doc) or directly production\nBeing JSON-RPC focused enables to have to focus on the \"commands\" which literally means your own business and not any technical concerns. Concretely it means that if you need to add a feature you just do by adding a command. It also encourages atomic commands and the caller/consumer to orchestrate the commands thanks to default or an advanced bulk endpoint. This is a big difference with REST: you do not need to do any custom hack to have an \"N application requests over 1 HTTP request\", it is built in and works with any language: C, Java, Javascript, Go...\nit does not prevent you to optimise some bulk pattern when needed by replacing the multiple atomic calls by a single one if relevant later.\nBeing based on HTTP and JSON it is quite easy to integrate with any technology, and it is quite optimised as of today.\nUsing living documentation, you always have an up-to-date view of your deployment, configuration (environment variables for example), etc... No more \"but in the documentation is not this name\".\nFinally, everything is testable, even the deployment, avoiding most of the surprises you can get at any stage of the pipeline.\nIf you are lost you can get started with a small generator^ .\nYou pick the features you want to it will generate you a ZIP you can download and run.\nFrom the same author:\n\nRomain Manni-Bucau\nIn the same categories:\nTechnology\nInfrastructure\nPrevious\nAll posts\nNext","title":"Yupiik Stack: deliver, deliver, deliver!","url":"/blog/yupiik-stack.html"},{"lang":"en","lvl2":"What's new?\nHow to start?\nFeedback are welcome","text":", Francois Papon , 2023-03-13, 38 sec read\nEnable to not index minisite pages (for search) by configuration\nAdd simple-dependencies mojo to export dependencies of a module in JSON in a file\nThis release include fixes and improvements, more details are available here:\nThe documentation is up-to-date here:\nYou can start be reading the Maven starter page .\nYou can contribute, report bug or ask for improvement in the GitHub issue page\nEnjoy!\nFrom the same author:\n\nFrancois Papon\nIn the same category:\nRelease\nPrevious\nAll posts\nNext","title":"Yupiik Tools 1.1.4 Released","url":"/blog/release-yupiik-tools-1.1.4.html"},{"lang":"en","lvl2":"What is Yupiik UShip\nGet started\nConclusion","text":", Romain Manni-Bucau , 2022-05-11, 4 min and 17 sec read\nYou get a new need, you do a new REST API...but does it scale? Is it maintainable? UShip intends to make it future ready and more customizable.\nYupiik UShip (µShip) is a light service framework. What does it mean? It means that it will enable you to expose a feature over HTTP - by default at least.\nConcretely the stack is pretty common and straight forward:\nA web server - Servlet and even a Tomcat by default,\nAn IoC - CDI and OpenWebBeans for the implementation,\nA (de)serialization format - JSON with JSON-P/JSON-B for the API and Johnzon for the implementation,\nA command API - JSON-RPC.\nThat is it.\nThe highlights on these choices are:\nThe stack relies on standards and are portables - i.e. you can switch Tomcat for Jetty if you want, OpenWebBeans for Weld and Johnzon for Yasson for example. So no vendor lock-in there and future proof choices.\nThe business is develop as JSON-RPC commands.\nThe last point hides a lot of goodness, and without rewriting JSON RPC post, it enables to:\nThink about commands and not HTTP requests,\nEnrich easily the protocol with custom features since it is JSON based and not with a custom grammar like GraphQL (like enabling to propagate some response to the next request in a bulk, see JSON RPC for an example),\nDrop HTTP in favor of another transport like Kafka messages without changing your business code/JSON-RPC methods,\nIntegrate with any other service in any language (JSON is well supported in all languages with a lot of libraries),\nWell document your API with no investment (UShip provides a AsciidoctorJsonRpcDocumentationGenerator which can generate an asciidoc documentation for your endpoint which can be rendered as HTML/PDF and even integrated with Yupiik Minisite ) or in a very fancy and customized fashion,\nWell share your specification thanks to OpenRPC format (equivalent to OpenAPI for JSON-RPC).\nUShip also covers all the layers of the development since:\nIt is compatible with openwebbeans-junit5 for the testing (more on the testing documentation),\nIt runs as a plain standalone Java application (java -cp ... CDILauncher) so is compatible with an assembly packaging and JIB to create docker images (more on the packaging documentation). Indeed you can also do a fat jar but as for any application it is not really recommended - but for unrelated reasons to UShip.\nthis part is a very light overview of getting started page.\nThe first step to start writing a UShip service is to import jsonrpc-core module:\nThen you can create a META-INF/beans.xml file in your resources (can be empty or <trim/> depending what you prefer for your CDI scanning) to enable CDI and write a JSON-RPC endpoint:\nTo start the application, you can write your own main using CDI SE API or use the default one of openwebbeans-se: org.apache.openwebbeans.se.CDILauncher. By default the container will start and stop but to await tomcat you can specify this arg: --openwebbeans.main uShipTomcatAwait. So overall you command will look like: java -cp .... org.apache.openwebbeans.se.CDILauncher --openwebbeans.main uShipTomcatAwait.\nOnce executed, you can call your endpoint:\nIf you have multiple commands and want to call multiple ones at the same time, you can use the bulk option of JSON-RPC:\nThis post just covers the basic steps to use UShip but the benefits are quite high (don't hesitate to read the previous links to understand them if you didn't already do).\nCustomizing the JSON-RPC bulk behavior is really worth it, it enables to control the size of the payload (making the network transfer faster), to compose commands in a flexible ways (giving clients more control about what they fetch and \"bulk\") and avoid to code a lot of new REST endpoints each time a new consumer/front has a new need making your API more atomic and focused.\nBy default it is still HTTP/Servlet based so you can still reuse all the goodness you are used to like HTTP headers for security, JWT validation etc... Being command based does not mean you throw away all the rest, just that your business code is command oriented and in case of migration to another transport you will keep your command and likely just replace the injected beans representing your security context thanks to CDI if you need such a bean in your business code (for a fine resource based security for example).\nThe last note is that Yupiik Logging is a nice companion for UShip making it even more cloud and Kubernetes friendly so don't hesitate to use both at the same time.\nFrom the same author:\n\nRomain Manni-Bucau\nIn the same category:\nTechnology\nPrevious\nAll posts\nNext","title":"Yupiik UShip Introduction","url":"/blog/uship-introduction.html"}]